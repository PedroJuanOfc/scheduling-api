import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config import get_settings

settings = get_settings()

DOCUMENTS_DIR = "./documents"
CHROMA_DIR = "./chroma_db"

vectorstore = None


def get_embeddings():
    """Retorna o modelo de embeddings da OpenAI."""
    return OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=settings.openai_api_key
    )


def load_and_index_documents():
    """Carrega PDFs e cria o √≠ndice vetorial."""
    global vectorstore
    
    if not settings.openai_api_key:
        return {"success": False, "message": "OPENAI_API_KEY n√£o configurada"}
    
    # Carregar todos os PDFs
    documents = []
    for filename in os.listdir(DOCUMENTS_DIR):
        if filename.endswith(".pdf"):
            filepath = os.path.join(DOCUMENTS_DIR, filename)
            print(f"üìÑ Carregando: {filename}")
            loader = PyPDFLoader(filepath)
            documents.extend(loader.load())
    
    if not documents:
        return {"success": False, "message": "Nenhum PDF encontrado na pasta documents/"}
    
    # Dividir em chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(documents)
    print(f"üìù Dividido em {len(chunks)} chunks")
    
    # Criar embeddings e indexar
    print("üîÑ Gerando embeddings com OpenAI...")
    embeddings = get_embeddings()
    
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_DIR
    )
    
    print("‚úÖ Documentos indexados!")
    return {"success": True, "message": f"Indexados {len(chunks)} chunks"}


def ask_question(question: str) -> dict:
    """Faz uma pergunta baseada nos documentos."""
    global vectorstore
    
    if not settings.openai_api_key:
        return {"success": False, "answer": "OPENAI_API_KEY n√£o configurada"}
    
    if vectorstore is None:
        try:
            embeddings = get_embeddings()
            vectorstore = Chroma(
                persist_directory=CHROMA_DIR,
                embedding_function=embeddings
            )
        except:
            return {"success": False, "answer": "√çndice n√£o encontrado. Execute /clinica/reindex primeiro."}
    
    # Buscar documentos relevantes
    docs = vectorstore.similarity_search(question, k=3)
    
    if not docs:
        return {"success": False, "answer": "N√£o encontrei informa√ß√µes sobre isso."}
    
    # Montar contexto
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # Perguntar ao GPT-4o-mini
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        openai_api_key=settings.openai_api_key
    )
    
    prompt = f"""Baseado no contexto abaixo, responda a pergunta de forma clara e amig√°vel.
Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o tem essa informa√ß√£o.

CONTEXTO:
{context}

PERGUNTA: {question}

RESPOSTA:"""
    
    response = llm.invoke(prompt)
    
    return {"success": True, "answer": response.content}